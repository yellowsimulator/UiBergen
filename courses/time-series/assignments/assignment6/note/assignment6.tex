\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{float}
\usepackage{bm}

\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\Cov}{\operatorname{Cov}}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal


\usepackage{scalerel,stackengine}
\stackMath
\newcommand\reallywidehat[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}%
    {\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
  }{\textheight}% 
}{0.5ex}}%
\stackon[1pt]{#1}{\tmpbox}%
}
\parskip 1ex


\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
     %frame=single, % adds a frame around the code
     backgroundcolor=\color{lightgray},
}
\usepackage[svgnames]{xcolor}
\title{STAT211 Mandatory Homework 6}
\author{Yapi Donatien Achou}
%\date{}							% Activate to display a given date or no date

\begin{document}

\maketitle 
\tableofcontents
\newpage
 
 
 \section{Problem 6.2}
 \subsection{Part a: Polynomial roots of moving average model}
 The model
 \begin{equation}
 X_{t} = Z_{t} + Z_{t-2}
 \end{equation}
 can be rewritten as 
 \begin{equation}
 X_{t} = Z_{t} + 0Z_{t-1} + 1Z_{t-2},
 \end{equation}
 and the corresponding moving average polynomial is
 \begin{equation}
 \theta(z) = 1+ 0z + 1.z^{2} = 1 + z^{2},
 \end{equation}
 whose roots are 
 \begin{equation}
 z_{1} = i, \quad z_{2} = -i
 \end{equation}
 \subsection{Part b: Polynomial roots of moving average model}
 The corresponding moving average polynomial for the model
 \begin{equation}
 X_{t} = Z_{t}  -2\cos(w)Z_{t-1} + Z_{t-2}
 \end{equation}
  is 
 \begin{equation}
 \begin{split}
 \theta(z) &= 1-2\cos(w)z + z^{2}\\
 &=(z-\cos(w))^{2} - \cos(w)^{2} +1 \\
 &=(z-\cos(w))^{2} - (\cos(w)^{2} -1) \\
 &(z-\cos(w))^{2}  - (-\sin(w)^{2})\\
 &(z-\cos(w))^{2}  - (i^{2}\sin(w)^{2})\\
 &(z-\cos(w))^{2}  - (i\sin(w))^{2}\\
&(z-\cos(w) + i\sin(w))(z-\cos(w) - i\sin(w))
 \end{split}
 \end{equation}
 whose roots are 
 \begin{equation}
 z_{1} = \cos(w) -i\sin(w), \quad  z_{2} = \cos(w) +i\sin(w)
 \end{equation}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 
 \section{Problem 6.3}
 Consider a causal AR(2) model with 
 \begin{equation}
 \{Z_{t} \} \sim WN(0,\sigma^{2})
  \end{equation}
 The two step predictor $\widehat{X}_{n+2}$ is defined by $\mathcal{P}_{n}(X_{n+2})$. From \cite{petter}, page 65 property 1,
 we have 
 \begin{equation}\label{eq:pn}
\mathcal{P}_{n}(X_{n+h}) = \sum_{i=1}^{n}a_{i}X_{n+1-i}
\end{equation} 
 where the $a_{i}$ satisfy
 \begin{equation}
 \Gamma_{n}\textbf{a}_{n} = \gamma_{n}(h), \quad \text{equation 2.5.7 from \cite{petter}},
 \end{equation}
 where
 \begin{equation}
 \textbf{a}_{n} = (a_{1}, \cdots,a_{n})
 \end{equation}
 
 \begin{equation}
 \Gamma_{n} = [\gamma(i-j) ]_{i,j=0}^{n}
 \end{equation}
 and 
 \begin{equation}
 \gamma_{n}(h) = (\gamma(h), \gamma(h+1),\cdots,\gamma(h+n-1)
 \end{equation}
 
 \begin{equation}
 \gamma(h) = \Cov(X_{t+h},X_{t}).
 \end{equation}
 To compute $\mathcal{P}_{n}(X_{n+2})$, we set $h = 2$ in equation (\ref{eq:pn}) and compute the coefficient $a_{i}$ by solving 
 \begin{equation}
 \Gamma_{n}\textbf{a}_{n} = \gamma_{n}(2)
 \end{equation}
 or more generally
 
 \begin{equation}
  \underbrace{
  \begin{pmatrix}
  \gamma(0) & \gamma(1) & \cdots & \gamma(n) \\
  \gamma(1) & \gamma(0) & \cdots & \gamma(n-1) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \gamma(n) & \gamma(n-1) & \cdots & \gamma(0) 
 \end{pmatrix}
 }_{\Gamma_{n}}
 \underbrace{
  \begin{pmatrix}
 a_{1}\\
 a_{2}\\
 \vdots\\
 a_{n}
 \end{pmatrix}
 }_{\textbf{a}_{n}}
  = 
  \underbrace{
  \begin{pmatrix}
 \gamma(2)\\
 \gamma(3)\\
 \vdots\\
 \gamma(n+1)
 \end{pmatrix}
  }_{\gamma_{n}(2)}
 \end{equation}
 To evaluate $\gamma(h)$, we note that the process is causal, which leads to 
 \begin{equation}
 X_{t} = Z_{t} + \psi_{1}Z_{t-1} +\psi_{2}Z_{t-2} 
 \end{equation}
 and
 \begin{equation}
 \begin{split}
 \gamma(h) &= \Cov(X_{t+h},X_{t})\\
 &=\Cov( Z_{t+h} + \psi_{1}Z_{t+h-1} +\psi_{2}Z_{t+h-2} , Z_{t} + \psi_{1}Z_{t-1} +\psi_{2}Z_{t-2} )\\
 &=\Cov(Z_{t+h},Z_{t})+\psi_{1}\Cov(Z_{t+h},Z_{t-1})+\psi_{2}\Cov(Z_{t+h},Z_{t-2})\\
 &+\psi_{1}\Cov(Z_{t+h-1},Z_{t})+\psi_{1}^{2}\Cov(Z_{t+h-1},Z_{t-1})+\psi_{1}\psi_{2}\Cov(Z_{t+h-1},Z_{t-2}) \\
 &+\psi_{2}\Cov(Z_{t+h-2},Z_{t})+\psi_{2}\psi_{1}\Cov(Z_{t+h-2},Z_{t-1})+\psi_{2}^{2}\Cov(Z_{t+h-1},Z_{t-2}) \\
 &=\sigma^{2}(\delta_{h,0} + \psi_{1}\delta_{h,-1} + \psi_{2}\delta_{h,-2} +\psi_{1}\delta_{h,1} +\psi_{1}^{2}\delta_{h,0} + \psi_{1}\psi_{2}\delta_{h,-1}+\psi_{2}\delta_{h,2}+\psi_{2}\psi_{1}\delta_{h,1}+\psi_{2}^{2}\delta_{h,-1})
 \end{split}
 \end{equation}
 From which we get
\begin{equation}
\begin{split}
\gamma(0)&=\sigma^{2}(1+\psi_{1}^{2})\\
\gamma(1)&=\sigma^{2}(\psi_{1}+\psi_{1}\psi_{2})\\
\gamma(2)&=\sigma^{2}\psi_{2}\\
\gamma(n)&= 0 , \quad \text{for n $\geq 3$}\\\
\end{split}
 \end{equation}
 
 Now the variance of $\mathcal{P}_{n}(X_{n+2})$ is given by 
 \begin{equation}
 \begin{split}
 \Var(\mathcal{P}_{n}(X_{n+2})) &= \Var\left(   \sum_{i=1}^{n}a_{i}X_{n+1-i}   \right)\\
 &=\sum_{i,j=1}^{n}a_{i}a_{j}\Cov(X_{n+1-i},X_{n+1-j})
 \end{split}
 \end{equation}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 
 
 \section{Problem 6.4}
 Let $\{ X_{t} \}$ be a stationary and linear causal time series with white noise process $\{ Z_{t} \} \sim WN(0,\sigma^{2})$. Let $\mathcal{P}_{n}$ be the projection onto $\{ X_{1},\cdots,X_{n} \}$.
 Let compute $\widehat{Z}_{n+1} = \mathcal{P}_{n}(Z_{n+1})$ and  $\widehat{Z}_{n} = \mathcal{P}_{n}(Z_{n})$
 Since $\{ X_{t} \}$ is linear we can write 
 \begin{equation}
 X_{t} = \sum_{j=0}^{n}\psi_{j}Z_{t-j}.
 \end{equation}
 From Problem 6.3  equation (\ref{eq:pn}) we have 
 \begin{equation}\label{eq:pn1}
\mathcal{P}_{n}(Z_{n+1}) = \sum_{i=1}^{n}a_{i}Z_{n+1-i}
\end{equation} 
and 
 \begin{equation}\label{eq:pn2}
\mathcal{P}_{n}(Z_{n}) = \sum_{i=1}^{n}b_{i}Z_{n-i}
\end{equation} 
where the  $a_{i}$ and $b_{i}$ are solution of 
 \begin{equation}
 \Gamma_{n}\textbf{a}_{n} = \gamma_{n}(1)
 \end{equation}
  \begin{equation}
 \Gamma_{n}\textbf{b}_{n} = \gamma_{n}(0)
 \end{equation}
respectively. Or 
  \begin{equation}
\textbf{a}_{n} = \Gamma_{n}^{-1} \gamma_{n}(1)
 \end{equation}
  \begin{equation}
\textbf{b}_{n} =  \Gamma_{n}^{-1}\gamma_{n}(0)
 \end{equation}

  \begin{equation}
 \underbrace{
  \begin{pmatrix}
 a_{1}\\
 a_{2}\\
 \vdots\\
 a_{n}
 \end{pmatrix}
 }_{\textbf{a}_{n}}
  = 
  \begin{pmatrix}
  \gamma(0) & \gamma(1) & \cdots & \gamma(n) \\
  \gamma(1) & \gamma(0) & \cdots & \gamma(n-1) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \gamma(n) & \gamma(n-1) & \cdots & \gamma(0) 
 \end{pmatrix}^{-1}
  \underbrace{
  \begin{pmatrix}
 \gamma(1)\\
 \gamma(2)\\
 \vdots\\
 \gamma(n+1)
 \end{pmatrix}
  }_{\gamma_{n}(1)}
 \end{equation}
 
 
 \begin{equation}
 \underbrace{
  \begin{pmatrix}
 b_{1}\\
 b_{2}\\
 \vdots\\
 b_{n}
 \end{pmatrix}
 }_{\textbf{b}_{n}}
  = 
  \begin{pmatrix}
  \gamma(0) & \gamma(1) & \cdots & \gamma(n) \\
  \gamma(1) & \gamma(0) & \cdots & \gamma(n-1) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \gamma(n) & \gamma(n-1) & \cdots & \gamma(0) 
 \end{pmatrix}^{-1}
  \underbrace{
  \begin{pmatrix}
 \gamma(1)\\
 \gamma(2)\\
 \vdots\\
 \gamma(n+1)
 \end{pmatrix}
  }_{\gamma_{n}(0)}
 \end{equation}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem 6.5}
let $\{Z_{n}\} \sim WN(0,1)$ and 
\begin{equation} \label{eq:p}
X_{t} = Z_{t} - Z_{t-1}
\end{equation}

\subsection{Part a}
The corresponding polynomial of model (\ref{eq:p}) is
\begin{equation}
\theta(z) = 1-z,
\end{equation}
whose root is
\begin{equation}
z = 1
\end{equation}
The root is on the unit circle. Since the root is not outside the unit circle, the process is not invertible.
\subsection{Part b}
Let try to create different representation of  the model in equation (\ref{eq:p}).
(\ref{eq:p}) we have 
\begin{equation} \label{eq:p1}
X_{t-1} = Z_{t-1} - Z_{t-2} \Rightarrow Z_{t-1} = X_{t-1} + Z_{t-2}
\end{equation}
and inserting the last expression back into (\ref{eq:p}) we get 
\begin{equation} \label{eq:pqw}
X_{t}+x_{t-1} = Z_{t} - Z_{t-2}.
\end{equation}
If we repeat the same process we get the following representation.
\begin{equation} \label{eq:p}
X_{t} +X_{t-1} + X_{t-2}= Z_{t} - Z_{t-3}.
\end{equation}
which is an ARMA(2,3), and the autoregressive polynomial 
\begin{equation} \label{eq:p00}
1+z+z^{2} = \left(z+\frac{1}{2}+ i\frac{\sqrt{3}}{2}\right) \left(z+\frac{1}{2}- i\frac{\sqrt{3}}{2}\right)
\end{equation}
has root on the unit circle. So It seams like through this process, it is not possible to create a representation which is invertible. But We also know that for each covariance function of a MA(q) process, there
exists one set of coefficients $d_{1}, \cdots, d_{q}$ such that the process is invertible. If we find this coefficients then we can have an invertible representation.

\subsection{Part c}
 using DL to find $\widehat{X}_{n+1}$ for $n = 1,2,3.$. The Durbin-Levinson recursion gives the coefficients of $X_{n}, \cdots, X_{1}$ in the following representation \cite{petter},
 \begin{equation}
 \widehat{X}_{n+1} = \sum_{j=1}^{n}\phi_{nj}X_{n+1-j}.
 \end{equation}
 We compute $\gamma(h)$ as follow:
 \begin{equation}
\begin{split}
\gamma(h) &= \Cov(X_{t+h}, X_{t})\\
&= \Cov(Z_{t+h} - Z_{t+h-1}, Z_{t} - Z_{t-1})\\
&=\Cov(Z_{t+h},Z_{t})-\Cov(Z_{t+h},Z_{t-1})-\Cov(Z_{t+h-1},Z_{t})+\Cov(Z_{t+h-1},Z_{t-1})\\
&=\sigma^{2}(\delta_{h,0}-\delta_{h,-1}-\delta_{h,1}+\delta_{h,0})\\
&=(\delta_{h,0}-\delta_{h,-1}-\delta_{h,1}+\delta_{h,0})
\end{split}
 \end{equation}
 and 
 \begin{equation}
 \phi_{11} = \frac{\gamma(1)}{\gamma(0)} = -\frac{1}{2}
 \end{equation}
 For $n=1$
 \begin{equation}
 \widehat{X}_{2} = \phi_{11}X_{1} = -\frac{1}{2}X_{1}
 \end{equation}
 For n = 2
 \begin{equation}
 \begin{split}
 \widehat{X}_{3} &= \sum_{j=1}^{2}\phi_{2j}X_{3-j}\\
 &=\phi_{21}X_{2} + \phi_{22}X_{1}
 \end{split}
 \end{equation}
 where 
 \begin{equation}
 \begin{split}
 \phi_{22} &= \frac{\gamma(2)-\phi_{11}\gamma(1)}{\nu_{1}}\\
 &=\frac{-\phi_{11}\gamma(1)}{\nu_{1}}\\
 \end{split}
 \end{equation}
 and 
 \begin{equation}
 \begin{split}
 \nu_{1} &= \nu_{0}(1-\phi_{11}^{2})\\
 &=\gamma(0)\left(1-\left(\frac{\gamma(1)}{\gamma(0)}\right)^{2}\right)\\
 &=2\left(1-\frac{1}{4}\right)\\
 &= \frac{3}{2}
 \end{split}
 \end{equation}
 so 
 \begin{equation}
 \begin{split}
 \phi_{22} &=\frac{-\phi_{11}\gamma(1)}{\nu_{1}}\\
 &= -\frac{1}{3}
 \end{split}
 \end{equation}
 
 \begin{equation}
 \begin{split}
 \phi_{21} &= \phi_{11} - \phi_{22}\phi_{11}\\
 &= -\frac{1}{2}-\frac{1}{3}\frac{1}{2}\\
 & = -\frac{2}{3}
 \end{split}
 \end{equation}
 
 \begin{equation}
 \begin{split}
  \widehat{X}_{3}&=\phi_{21}X_{2} + \phi_{22}X_{1}\\
  &=-\frac{2}{3}X_{2}-\frac{1}{3}X_{1}
  \end{split}
 \end{equation}
 
for n = 3

\subsection{Part d}
let prove that 
\begin{equation}
\widehat{X}_{n+1} = -\sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j}.
\end{equation}
We use induction. Thats is we show that its true for $n = 1$, then we show that it is true for $n+1$. Let go:
for $n = 1$
\begin{equation}
\begin{split}
 \widehat{X}_{2} &= -\frac{1}{2}X_{1}\\
 &=-\frac{1+1-1}{1+1}X_{1+1-1}\\
 &=-\sum_{j=1}^{n=1}\frac{n+1-j}{n+1}X_{n+1-j}\\
 \end{split}
 \end{equation}

Now let j run from 1 to $n+1$. Then we have 
 \begin{equation}
 \begin{split}
\widehat{X}_{n+1} &= -\sum_{j=1}^{n+1}\frac{n+1-j}{n+1}X_{n+1-j}\\
&= -\sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j} -\sum_{j=n+1}^{n+1}\frac{n+1-j}{n+1}X_{n+1-j}\\
&= -\sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j} -\underbrace{\frac{n+1-(n+1)}{n+1}}_{ = 0}X_{n+1-(n+1)}\\
&=-\sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j}
\end{split}
\end{equation}

\subsection{Part e}
Let prove that 
\begin{equation}
|| Z_{n}- (-\widehat{X}_{n+1})|| = O(1).
\end{equation}
Now 
\begin{equation}
\begin{split}
\left|\left| Z_{n}- \sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j}\right|\right| &= \left|\left| Z_{n}- \left(\frac{n}{n+1}X_{n}+ \cdots+\frac{2}{n+1}X_{2}+ \frac{1}{n+1}X_{1}\right)\right|\right|
\end{split}
\end{equation}
Now as n goes to $\infty$, $\left(\frac{n}{n+1}X_{n}+ \cdots+ \frac{1}{n+1}X_{1}\right)$ =  $\left(\frac{1}{1+\frac{1}{n}}X_{n}+ \cdots+ \frac{1}{n+1}X_{1}\right)$ goe to $X_{n}$ so 
$\left|\left| Z_{n}- \sum_{j=1}^{n}\frac{n+1-j}{n+1}X_{n+1-j}\right|\right| $ goes to $|| Z_{n}-X_{n} ||$ = $|| Z_{n-1} ||$ which goes to 0

so that $Z_{n}$ can be written as a linear combination of $X_{s}$ so we can conclude that $Z_{t} \in span{X_{s}}$
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem 6.6}
Let $\mathcal{P}_{k}$ be the linear projection onto
\begin{equation}\label{eq:e}
\textbf{S}_{k} = span\{ X_{1},\cdots, X_{k}  \}
\end{equation}
and 
\begin{equation}\label{eq:e0}
e_{k} = \frac{X_{k}-\hat{X}_{k}}{\nu_{k-1}}.
\end{equation}
$\{e_{1},\cdots,e_{n}\}$
is orthonormal basis for $\textbf{S}_{n}$ if $\{e_{1},\cdots,e_{n}\}$ is a linearly independent subset of $\textbf{S}_{n}$ that span $\textbf{S}_{n}$, and for any $e_{j}, e_{i}$ in $\{e_{1},\cdots,e_{n}\}$ 
the inner product of $e_{j}$ and $e_{i}$ is zero and any $e_{i}$ as norm 1.
\justify
\begin{proof}
\begin{itemize}
\item Linearly independence.
%\justify
Assume that 
\begin{equation}
  a_{1}e_{1}+\cdots+a_{n}e_{n} = 0
  \end{equation}
  where $a_{i}$ are real numbers. The we have 
\begin{equation}\label{eq:e1}
\begin{split}
a_{1}e_{1}+\cdots+a_{n}e_{n} &= 0\\
a_{1}\frac{X_{1}-\hat{X}_{1}}{\nu_{0}}+\cdots+a_{n}\frac{X_{n}-\hat{X}_{n}}{\nu_{n-1}} &= 0\\
\frac{a_{1}}{\nu_{0}}(X_{1}-\hat{X}_{1}) + \cdots+ \frac{a_{n}}{\nu_{n-1}}(X_{n}-\hat{X}_{n}) &=0\\
%\frac{a_{1}}{\nu_{0}}X_{1} + \cdots+ \frac{a_{n}}{\nu_{n-1}}X_{n} &=\frac{a_{1}}{\nu_{0}}\hat{X}_{1} + \cdots+ \frac{a_{n}}{\nu_{n-1}}\hat{X}_{n}\\
\end{split}
\end{equation}
From (\ref{eq:e0}) we know that 
\begin{equation}
X_{k}-\hat{X_{k}} = e_{k}\nu_{k-1}.
\end{equation}
Thus 
\begin{equation}
X_{1} -\hat{X_{1}} \neq 0 , \cdots, X_{n}-\hat{X_{n}} \neq 0 
\end{equation} 
Therefor the last expression in equation (\ref{eq:e1}) is true if 
\begin{equation}
\frac{a_{1}}{\nu_{0}} = \cdots =\frac{a_{n}}{\nu_{n-1}} = 0
\end{equation} 
equivalently 
\begin{equation}
a_{1} = \cdots =a_{n} = 0
\end{equation}
This means that $\{e_{1},\cdots,e_{n}\}$ is a linearly independent
 
%\justify
\item $\{e_{1},\cdots,e_{n}\}$ span $\textbf{S}_{n}$. We want to show that any vector in $\textbf{S}_{n}$ can be written as a linear combination of $\{e_{1},\cdots,e_{n}\}$.
Let $Z \in \textbf{S}_{n}$. Since $\textbf{S}_{n} = span\{ X_{1},\cdots, X_{n}  \}$, we have 
\begin{equation}
Z = b_{1}X_{1} + \cdots + b_{n}X_{n}
\end{equation}
where $b_{i}$ are real numbers. Then we have 

\begin{equation}
\begin{split}
Z &= b_{1}X_{1} + \cdots + b_{n}X_{n}\\
Z&=b_{1}(\nu_{0}e_{1} + \hat{X}_{1}) + \cdots + b_{n}(\nu_{n-1}e_{n} + \hat{X}_{n})\\
Z&=b_{1}\nu_{0}e_{1}  + \cdots + b_{n}\nu_{n-1}e_{n} + \underbrace{b_{1}\hat{X}_{1} + \cdots + b_{n}\hat{X}_{n}}_{Z^{\prime}}\\
\underbrace{Z-Z^{\prime}}_{Z^{\prime\prime}}&=\underbrace{b_{1}\nu_{0}}_{\alpha_{1}}e_{1}  + \cdots + \underbrace{b_{n}\nu_{n-1}}_{\alpha_{n}}e_{n} 
\end{split}
\end{equation} 
Since $Z^{\prime\prime} \in \textbf{S}_{n}$
We have 
\begin{equation}
Z^{\prime\prime} = \alpha_{1}e_{1}  + \cdots + \alpha_{n}e_{n} 
\end{equation}

\item Horthogonality
Let $e_{r}, e_{r}$ be two arbitrarily vectors in $\{ e_{1}, \cdots, e_{n}   \}$ such that $r \neq s$.
\begin{equation}
\begin{split}
\langle\,e_{r},e_{s}\rangle & = \left\langle\,\frac{X_{r}-\hat{X}_{r}}{\nu_{r-1}} ,\frac{X_{s}-\hat{X}_{s}}{\nu_{s-1}}\right\rangle\\
&=\frac{1}{\nu_{r-1}\nu_{i-s}}\left\langle\,X_{r}-\hat{X}_{r} ,X_{s}-\hat{X}_{s}\right\rangle\\
\end{split}
\end{equation}
From the innovation algorithm \cite{petter}, the coefficient of $X_{n}-\hat{X}_{n}, \cdots, X_{1}-\hat{X}_{1}$ are of the form 
\begin{equation}
\theta_{n,n-k}, \quad k=0, \cdots, n.
\end{equation}

so that 
\begin{equation}
\begin{split}
\langle\,e_{r},e_{s}\rangle & = \left\langle\,\frac{X_{r}-\hat{X}_{r}}{\nu_{r-1}} ,\frac{X_{s}-\hat{X}_{s}}{\nu_{s-1}}\right\rangle\\
&=\frac{1}{\nu_{r-1}\nu_{i-s}}\left\langle\,X_{r}-\hat{X}_{r} ,X_{s}-\hat{X}_{s}\right\rangle\\
&= \frac{\theta_{r,r-k} \theta_{s,s-k}}{\nu_{r-1}\nu_{s-1}}
\end{split}
\end{equation}
And from \cite{petter} equation 2.5.26, for $r\neq s$ we get 
\begin{equation}
\theta_{r,r-k} \theta_{s,s-k} = 0, \Rightarrow \langle\,e_{r},e_{s}\rangle = 0
\end{equation}

\item Normality
The






\end{itemize}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem6.7}
Let $\{X_{t}\}$ be a stationary time series. Suppose that $\gamma(n) =O(1)$. Prove that this assumption is sufficient for $\Gamma_{n}$ to be non singular.

\begin{proof}
We know that 
\begin{equation}
\Gamma_{n} = 
\begin{pmatrix}
  \gamma(0) & \gamma(1) & \cdots & \gamma(n) \\
  \gamma(1) & \gamma(0) & \cdots & \gamma(n-1) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \gamma(n) & \gamma(n-1) & \cdots & \gamma(0) 
 \end{pmatrix}
\end{equation}
$\gamma(n) = O(1)$ means that as as n goes to infinity, $\gamma(n)$ goes to zero. This means that the $n\times n$ matrix $\Gamma_{n}$ is well defined. 
Since $\{X_{t}\}$ is a stationary time series, the best linear predictor in terms of $\{1, X_{n}, \cdots, X_{1}\}$ is \cite{petter},
\begin{equation}\label{eq:P0}
P_{n}X_{n+h} = a_{0} + a_{1}X_{n}+\cdots+a_{n}X_{1}
\end{equation}
where the $a_{i}$ are solution of \cite{petter},
\begin{equation}\label{eq:equation1}
\Gamma_{n}\textbf{a}_{n} = \gamma_{n}(h)
\end{equation}
Since $P_{n}X_{n+h}$ in (\ref{eq:P0}) exists, then the $a_{i}$ also are well defined, which means that the solution of (\ref{eq:equation1}) exists, thus $\Gamma_{n}$ is invertible, and
\begin{equation}\label{eq:equation2}
\textbf{a}_{n} = \Gamma_{n}^{-1}\gamma_{n}(h)
\end{equation}

\end{proof}
\begin{thebibliography}{9}
\bibitem{petter} 
Petter J. Brockwell. Richard A. Davis
\textit{Introduction to Time Series and Forecasting}. 
Springer. Second edition. 2001
 
\end{thebibliography}


\end{document}  