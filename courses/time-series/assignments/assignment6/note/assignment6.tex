\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{float}
\usepackage{bm}

\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\Cov}{\operatorname{Cov}}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
     %frame=single, % adds a frame around the code
     backgroundcolor=\color{lightgray},
}
\usepackage[svgnames]{xcolor}
\title{STAT211 Mandatory Homework 6}
\author{Yapi Donatien Achou}
%\date{}							% Activate to display a given date or no date

\begin{document}

\maketitle 
\tableofcontents
\newpage
 
 
 \section{Problem 6.2}
 \subsection{Part a}
 Root of 
 \begin{equation}
 X_{t} = Z_{t} + Z_{t-2}
 \end{equation}
 We can rewrite this as 
 \begin{equation}
 X_{t} = Z_{t} + 0Z_{t-1} + 1Z_{t-2}
 \end{equation}
 and the corresponding moving average polynomial is then 
 \begin{equation}
 \theta(z) = 1+ 0z + 1.z^{2} = 1 + z^{2}.
 \end{equation}
 whose roots are 
 \begin{equation}
 z_{1} = i, \quad z_{2} = -i
 \end{equation}
 \subsection{Part b}
 Root of 
 \begin{equation}
 X_{t} = Z_{t}  -2\cos(w)Z_{t-1} + Z_{t-2}
 \end{equation}
 and the corresponding moving average polynomial is 
 \begin{equation}
 \begin{split}
 \theta(z) &= 1-2\cos(w)z + z^{2}\\
 &=(z-\cos(w))^{2} - \cos(w)^{2} +1 \\
 &=(z-\cos(w))^{2} - (\cos(w)^{2} -1) \\
 &(z-\cos(w))^{2}  - (-\sin(w)^{2})\\
 &(z-\cos(w))^{2}  - (i^{2}\sin(w)^{2})\\
 &(z-\cos(w))^{2}  - (i\sin(w))^{2}\\
&(z-\cos(w) + i\sin(w))(z-\cos(w) - i\sin(w))
 \end{split}
 \end{equation}
 and the roots are 
 \begin{equation}
 z_{1} = \cos(w) -i\sin(w), \quad  z_{2} = \cos(w) +i\sin(w)
 \end{equation}
 
 \section{Problem 6.3}
 
 
\section{Problem 6.6}
Let $\textbf{P}_{k}$ be the linear projection onto
\begin{equation}\label{eq:e}
\textbf{S}_{k} = span\{ X_{1},\cdots, X_{k}  \}
\end{equation}
and 
\begin{equation}\label{eq:e0}
e_{k} = \frac{X_{k}-\hat{X}_{k}}{\nu_{k-1}}.
\end{equation}
$\{e_{1},\cdots,e_{n}\}$
is orthonormal basis for $\textbf{S}_{n}$ if $\{e_{1},\cdots,e_{n}\}$ is a linearly independent subset of $\textbf{S}_{n}$ that span $\textbf{S}_{n}$, and for any $e_{j}, e_{i}$ in $\{e_{1},\cdots,e_{n}\}$
the inner product of $e_{j}$ and $e_{i}$ is zero and any $e_{i}$ as norm 1.
\justify
\begin{proof}
\begin{itemize}
\item Linearly independence.
%\justify
Assume that 
\begin{equation}
  a_{1}e_{1}+\cdots+a_{n}e_{n} = 0
  \end{equation}
  where $a_{i}$ are real numbers. The we have 
\begin{equation}\label{eq:e1}
\begin{split}
a_{1}e_{1}+\cdots+a_{n}e_{n} &= 0\\
a_{1}\frac{X_{1}-\hat{X}_{1}}{\nu_{0}}+\cdots+a_{n}\frac{X_{n}-\hat{X}_{n}}{\nu_{n-1}} &= 0\\
\frac{a_{1}}{\nu_{0}}(X_{1}-\hat{X}_{1}) + \cdots+ \frac{a_{n}}{\nu_{n-1}}(X_{n}-\hat{X}_{n}) &=0\\
%\frac{a_{1}}{\nu_{0}}X_{1} + \cdots+ \frac{a_{n}}{\nu_{n-1}}X_{n} &=\frac{a_{1}}{\nu_{0}}\hat{X}_{1} + \cdots+ \frac{a_{n}}{\nu_{n-1}}\hat{X}_{n}\\
\end{split}
\end{equation}
From (\ref{eq:e0}) we know that 
\begin{equation}
X_{k}-\hat{X_{k}} = e_{k}\nu_{k-1}.
\end{equation}
Thus 
\begin{equation}
X_{1} -\hat{X_{1}} \neq 0 , \cdots, X_{n}-\hat{X_{n}} \neq 0 
\end{equation} 
Therefor the last expression in equation (\ref{eq:e1}) is true if 
\begin{equation}
\frac{a_{1}}{\nu_{0}} = \cdots =\frac{a_{n}}{\nu_{n-1}} = 0
\end{equation} 
equivalently 
\begin{equation}
a_{1} = \cdots =a_{n} = 0
\end{equation}
This means that $\{e_{1},\cdots,e_{n}\}$ is a linearly independent
 
%\justify
\item $\{e_{1},\cdots,e_{n}\}$ span $\textbf{S}_{n}$. We want to show that any vector in $\textbf{S}_{n}$ can be written as a linear combination of $\{e_{1},\cdots,e_{n}\}$.
Let $Z \in \textbf{S}_{n}$. Since $\textbf{S}_{n} = span\{ X_{1},\cdots, X_{n}  \}$, we have 
\begin{equation}
Z = b_{1}X_{1} + \cdots + b_{n}X_{n}
\end{equation}
where $b_{i}$ are real numbers. Then we have 

\begin{equation}
\begin{split}
Z &= b_{1}X_{1} + \cdots + b_{n}X_{n}\\
Z&=b_{1}(\nu_{0}e_{1} + \hat{X}_{1}) + \cdots + b_{n}(\nu_{n-1}e_{n} + \hat{X}_{n})\\
Z&=b_{1}\nu_{0}e_{1}  + \cdots + b_{n}\nu_{n-1}e_{n} + \underbrace{b_{1}\hat{X}_{1} + \cdots + b_{n}\hat{X}_{n}}_{Z^{\prime}}\\
\underbrace{Z-Z^{\prime}}_{Z^{\prime\prime}}&=\underbrace{b_{1}\nu_{0}}_{\alpha_{1}}e_{1}  + \cdots + \underbrace{b_{n}\nu_{n-1}}_{\alpha_{n}}e_{n} 
\end{split}
\end{equation} 
Since $Z^{\prime\prime} \in \textbf{S}_{n}$
We have 
\begin{equation}
Z^{\prime\prime} = \alpha_{1}e_{1}  + \cdots + \alpha_{n}e_{n} 
\end{equation}

\item Horthogonality
Let $e_{r}, e_{r}$ be two arbitrarily vectors in $\{ e_{1}, \cdots, e_{n}   \}$ such that $r \neq s$.
\begin{equation}
\begin{split}
\langle\,e_{r},e_{s}\rangle & = \left\langle\,\frac{X_{r}-\hat{X}_{r}}{\nu_{r-1}} ,\frac{X_{s}-\hat{X}_{s}}{\nu_{s-1}}\right\rangle\\
&=\frac{1}{\nu_{r-1}\nu_{i-s}}\left\langle\,X_{r}-\hat{X}_{r} ,X_{s}-\hat{X}_{s}\right\rangle\\
\end{split}
\end{equation}
From the innovation algorithm \cite{petter}, the coefficient of $X_{n}-\hat{X}_{n}, \cdots, X_{1}-\hat{X}_{1}$ are of the form 
\begin{equation}
\theta_{n,n-k}, \quad k=0, \cdots, n.
\end{equation}

so that 
\begin{equation}
\begin{split}
\langle\,e_{r},e_{s}\rangle & = \left\langle\,\frac{X_{r}-\hat{X}_{r}}{\nu_{r-1}} ,\frac{X_{s}-\hat{X}_{s}}{\nu_{s-1}}\right\rangle\\
&=\frac{1}{\nu_{r-1}\nu_{i-s}}\left\langle\,X_{r}-\hat{X}_{r} ,X_{s}-\hat{X}_{s}\right\rangle\\
&= \frac{\theta_{r,r-k} \theta_{s,s-k}}{\nu_{r-1}\nu_{s-1}}
\end{split}
\end{equation}
And from \cite{petter} equation 2.5.26, for $r\neq s$ we get 
\begin{equation}
\theta_{r,r-k} \theta_{s,s-k} = 0, \Rightarrow \langle\,e_{r},e_{s}\rangle = 0
\end{equation}

\item Normality
The






\end{itemize}
\end{proof}

\begin{thebibliography}{9}
\bibitem{petter} 
Petter J. Brockwell. Richard A. Davis
\textit{Introduction to Time Series and Forecasting}. 
Springer. Second edition. 2001
 
\end{thebibliography}


\end{document}  