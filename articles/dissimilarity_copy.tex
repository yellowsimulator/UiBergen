\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
%SetFonts
\usepackage{tikz}
\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{blindtext}
%\usepackage{romannum}
\usepackage[utf8]{inputenc}
\newcommand{\Var}{\operatorname{Var}}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
\usepackage{enumitem}
% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{pgfplots}

\usepackage{pst-plot}
\psset{algebraic}

\usepackage{listings}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proof}{Proof}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\title{Machine learning and signal processing methods for anomaly detection in predictive maintenance (Project outline)}
\author{Yapi Donatien Achou}
%\date{}							% Activate to display a given date or no date

\begin{document}


\maketitle
\tableofcontents
\newpage
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=5in]{front4} 
%   \caption{}
%   \label{fig:example}
%\end{figure}
\section{Introduction}
Since the industrial revolution, engines and machines have been the driving force for economical growth across industries such as automotive, airline, oil and gaze, to name a few. However, machines are prone to failure and must be monitor and maintain regularly to avoid catastrophic failure leading to significant financial and human lost. To mitigate equipment and machines proclivity toward failure and the associated cost, a process called predictive maintenance has been developed within the industrial community. Predictive maintenance  for machines and industrial equipments can be defined as a maintenance philosophy or more generally a framework with a set of methods used to predict and prevent machine failure in order to avoid unexpected downtime and reduce related human and financial cost. This maintenance philosophy, when correctly implemented, increases machine life time, and reduces maintenance cost [reference].
\justify
As a framework, predictive maintenance has 4 levels [reference]. In level 1, visual inspection of machines or equipments are performed in order to assess any damage.
In addition, equipment must fail before they are replaced, which can incur a high production cost. In level 2 and 3, machines characteristics such as vibration, temperature, electrical current, voltage etc are monitored continually or periodically depending on their criticality. This is called condition monitoring. In condition monitoring, the goal is to detect any change in machine normal behaviour, in order to detect failure as early as possible and schedule maintenance accordantly. Maintenance actions are performed periodically regardless of machine health condition. In level 4, big data and machine learning are the main driving forces in detecting failure and planning maintenance. At this level, maintenance action are not performed periodically but are planned according to machine health condition derived from the application of anomaly detection techniques. This reduce unnecessary maintenance action and significantly cut down maintenance cost as well as increasing machine life time.
\justify
According to PriceWaterhouseCoopers (PWC), one of the four largest auditing and consulting company in the world,  a survey from 280 companies in Belgium, Germany and the Nederland, revealed that only 
$11 \%$ of companies have reached level 4 \cite{pwc}. The application of level 4 requires collecting, saving and analysing large amount of data, from which maintenance decision can be made. Anomaly detection methods and more generally supervised and unsupervised learning methods are used in the analysis phase.
%These methods, detect early sign of anomalies such as machines defect and predict their remaining useful life, which is the time left before catastrophic break down. 
\justify
In supervised learning, based on available failure data from defect machine, a learning algorithm is trained to recognise the failure pattern in the data. This is sometime achieved by fitting the algorithm parameters to the data, which result in a model called a classifier or a regressor. A classifier is a model derived from a classification algorithm while a regressor is a model derived from a regression algorithm.
\justify
 In machine learning, the failure data is called a labeled data because we can assign a categorical label such as fail or a numerical label such as 1 to specify the condition of the machine through the data measuring its characteristics. In the absence of labeled data, unsupervised learning methods can be apply to detect pattern in the data without prior knowledge to classify data regions as anomalous or not. One such technique is clustering algorithm where the input data is separated into subregions.
\justify
The procedure of classifying a data region as anomalous or not is called anomaly detection. An anomaly is defined as a pattern in the data, that does not conform to expected normal behaviour \cite{chandola}. A general anomaly detection strategy will first detect normal behaviour, secondly set a boundary around the normal behaviour and finally declare any data out of the boundary as anomaly.
\justify
Several factor make this general approach of anomaly detection challenging: The notion of anomaly is different for different application domain and not all application domain have enough labeled data to train a supervise learning algorithm \cite{chandola}. An extensive survey from \cite{chandola} revealed that the majority of research have been focussing on simple anomalies while most application domain are faced with complex anomalies. There are mainly three types of anomalies: Point anomalies which are simple anomalies, contextual  and collective anomalies which are complex anomalies \cite{chandola}. When one data point in a time series is anomalous with respect to the other data points, we have a point anomaly. In a contextual anomaly, a data instance is anomalous relative to a context. For example the vibration of a machine might be very high if the load increase suddenly, and decrease when the load goes back to normal. But if the vibration increases monotonously regardless of the load, then we have a contextual anomaly. If a collection of related data instance is anomalous with respect to the entire data set, it is termed a collective anomaly \cite{chandola}.


\begin{flushleft}
In rotating machines, more than 40 $\%$ of anomalies can be attributed to bearing defect [references]. Figure \ref{fig:pie} shows the failure statistics for rotating machines. 
\end{flushleft}
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{pie.png} 
   \caption{Defect statistics for rotating machines, taken from [reference]}
   \label{fig:pie}
\end{figure}
In this project we present a mixed methodology to detect and predict bearing defects. The methodology consists of using signal processing for feature generation and data labelling,  and machine learning for defects classification and failure prediction. For a given input dataset, the dataset is decomposed into it subcomponents or basis components. The basis components also called features or attributes are further used as input of a supervised learning algorithm for defect classification and failure prediction.
\begin{flushleft}
The signal processing methods used are Fourier transform, wavelet transform and Hilbert Huang transform. We focus on ensemble learning and feed forward neural network for classification. Furthermore we show that the back-propagation process in the feed forward neural network can be modelled by an ordinary differential equation, whose solution represents the path of the hidden and output layer weights.
\end{flushleft}
\justify
This thesis is structured into six parts. To put the current work in prospective, chapter two outlines preview works, where machine learning and/or signal processing have been applied to anomaly detection for machines fault detection. Since the latter is the main goal, chapter three presents a general anomaly detection methodology, followed by a proposed methodology for predictive maintenance. The main methods used for anomaly detection, that is signal processing and machine learning are presented in chapter four and five respectively. The capstone of this work, to say the least, is revealed in chapter six in a form of three case studies, where we apply our proposed methodology to solve up to date anomaly detection problems encounter in a wide range of industries.
As in any work, we conclude this thesis by summarising what have been done. The summary includes the strengths and weaknesses of our methodology, as well as what could be done to improve upon it.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Literature review  \textcolor{red}{(TO DO!!)})}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Anomaly detection methodology and methods \textcolor{red}{(TO DO!!)}}
\subsection{General anomaly detection methodology   \textcolor{red}{(TO DO!!)}}
\subsection{A proposed methodology for predictive maintenance   \textcolor{red}{(TO DO!!)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Signal processing methods}
\subsection{Overview}
 In this section, we present three signal processing methods to generate new features also called attributes, for anomaly detection. In the context of this work, a feature or attribute is a numerical sequence that represents the state of a system. For example, voltage and vibration measurement taken from a motor over time are time series that we call features. These two features contain information about the state of the motor, the latter being the system in this case. 
 By state we mean the current health of the motor. The health of the motor is represented by a health index which is a numerical value that quantifies the overall condition of the motor.
 \justify
 Generating new features from existing one, is a common procedure for anomaly detection. For example, by using Fourier transform, we can generate the frequency spectrum of a vibration time series. The frequency spectrum is the set of all frequencies from each vibration component of the original vibration time series. The importance of the frequency spectrum, lies in the fact that if a motor or machine is anomalous, the anomaly will generate an extra component in the vibration time signal, and the corresponding anomalous frequency will be visible in the frequency spectrum. Due to Fourier transform limitations, other signal processing techniques such as
  wavelet and Hilbert Huang transform are alternatives for generating new features to deal with more complex time series for anomaly detection.
 \justify
 In the rest of this section, we give an expos$\acute{e}$  of three signal processing methods, namely: Fourier analysis, wavelet transform and Hilbert Huang transform. For each transform, we cover the theoretical back bone in terms of mathematical constructs such as basis, vector space, orthogonality, existence, uniqueness, to name a few. Furthermore, these mathematical constructs will set the limitations as well as the strengths of each transform. Following the theoretical set up is a concrete application, where we generate new features and show how they can be used for anomaly detection.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Fourier Analysis}
From solving differential equations to analysing sound wave, images and signal in general, Fourier analysis has a profound impact in science and engineering. It provides a convenient way to transform data from time domain to frequency domain, thus revealing unseen aspect of data. A time domain data can be viewed as a series of observations generated by a given process and recorded at discrete or continuous time interval. The underlying process might be the sum of subprocesses. In this case, the frequency domain will reveal all the subprocesses characteristics.

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fft_domain} 
   %\includegraphics[width=2.5in]{freq} 
   \caption{}
   \label{fig:pro}
\end{figure}
\justify
Figure \ref{fig:pro} Shows the time domain representation of the signal (in dark). The original signal is decomposed into four sub components (in blue) by using Fourier analysis. Each component frequencies and amplitude represent the frequency domain (in red). 
\justify
Fourier analysis is concerned with the general problem of periodic and non periodic functions approximation. Fourier series on one hand addresses the former, while the latter is treated by Fourier transform. Given a periodic function, its Fourier series is given as a discrete superposition of exponential functions, and its Fourier transform is given by continuous superposition of exponential functions.
\justify
Fourier analysis is used in a wide range of application, including signal processing, data compression, image analysis. The main objective is to take a signal or more generally a function, and decompose its in trigonometric functions. 
\subsubsection{Fourier series}

Looking at the bigger picture, Fourier series is concerned with the general problem of periodic functions approximation. The basic ingredients required to approximate a function in this scenario are: a vector space, a basis, which is a subspace of the vector space, and a mathematical operation, or more generally a function such as an inner product that maps two vectors to a real number. If a vector space has an inner product, we say that the vector space is an inner product space. 
\justify
Before continuing, we see the need to clarify some abbreviations. We use the letters $f$, $V$, $V_{0}$ for an arbitrary function, a vector space, and a subspace of a vector space, respectively.
Basis functions will be denoted by $\{  \varphi_{0}, \cdots,\varphi_{n}\}$, where $n$ can either be a finite integer or infinite. Having made this clarification, let explain the concept of function approximation. 
\justify
The function approximation process in light of Fourier series goes like this: Given an arbitrary function $f$ that we seek to approximate, we pick an appropriate vector space which we call $V$, such that $f\in V$. We define a subspace $V_{0}$ of the vector space $V$ and construct an inner product on $V_{0}$, if it does not exist. Furthermore, we fine an appropriate basis of $V_{0}$.  A basis of $V_{0}$ is a set of linearly independent vectors   $\{  \varphi_{0}, \cdots,\varphi_{n}\}$ in $V_{0}$, that span $V_{0}$. This means that any vector in $V_{0}$ can be written as a linear combination of the basis vectors. Once we have all this in place, the best approximation of the function $f$ is its orthogonal projection in the inner product space $V_{0}$. Figure \ref{figure:il} shows an illustration of a generic mechanism of function approximation by orthogonal projection, where $f_{0}$ is the orthogonal projection of $f$ in the subspace $V_{0}$ of $V$.
\justify
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
%\draw (0,0) -- (0,5) -- (0,0) -- (5,0) -- (5,5) -- (0,5);
%\draw (1,1) -- (1,4) -- (1,1) -- (4,1) -- (4,4) -- (1,4);
%\draw (0,0) rectangle (7,5);
%\draw (1,1) rectangle (6,4);
% reactagle 1
\path (0,0) coordinate (origin1);
\path(0.5,4.5) node (V){$V$};
\path(2.5,0.5) node (f){$\textcolor{blue}{f}\textcolor{blue} {\approx f_{0} = \sum_{j=1}^{n} \alpha_{j} \varphi_{j}}$};
\path (0,5) coordinate (topleft1);
\path (7,5) coordinate (topright1);
\path (7,0) coordinate (bottom1);
% draw
\draw (origin1) -- (bottom1) -- (topright1) -- (topleft1) -- (origin1);

% reactagle 2
\path (1,1) coordinate (origin2);
\path(1.5,3.5) node (V0){$V_{0}$};
\path(1.5,1.5) node (f0){$\textcolor{blue}{f_{0}}$};

\path (1,4) coordinate (topleft2);
\path (6,4) coordinate (topright2);
\path(4.5,3.5) node (b0){$ \{\varphi_{1}, \varphi_{2}, \cdots, \varphi_{n}\} $};
\path (6,1) coordinate (bottom2);
% draw
\draw (origin2) -- (bottom2) -- (topright2) -- (topleft2) -- (origin2);
\end{tikzpicture}
\end{center}
\caption{Illustration of a generic function approximation process of a function $f$ into a subspace $V_{0}$ of a vector space $V$. $\varphi_{j}$ are the basis functions and $\alpha_{j}$ are real numbers for $j=1,\cdots,n$.}
\label{figure:il}
\end{figure}
\justify
The Fourier space is a subspace of the space of all continuous functions of the interval $[0, T]$ and denoted by $C[0,T]$. Let $V_{N,T}$ be the subspace of $C[0,T]$ spanned by 
\begin{equation}
\left\{1, \cos\left( \frac{2\pi t}{T} \right), \cdots,\cos\left( \frac{2\pi Nt}{T} \right), \sin\left( \frac{2\pi t}{T} \right), \cdots,\sin\left( \frac{2\pi Nt}{T} \right)   \right\}.
\end{equation}
The space $V_{N,T}$ is called the $N-th$ order Fourier series space. The Fourier series representation of an arbitrary periodic function $f$ of period $T=2L$ defined on an interval of length $L$
is the best approximation of $f$ in $V_{N,T}$. It is the the orthogonal projection of $f$ into $V_{N,T}$ with respect to the inner product
\begin{equation}
<,f,g> = \frac{1}{T}\int_{0}^{T}f(t)g(t)\mathrm{d}t
\end{equation}






\justify
Let $f$ be an arbitrary periodic function of period $T=2L$, defined on an interval of length $L$. Its Fourier series representation is
\begin{equation}\label{eq:fs}
f(t) = \frac{a_{0}}{2} +\sum_{n=0}^{\infty}\left( a_{n} \cos\left( \frac{n\pi t}{L}\right) + b_{n} \sin\left( \frac{n\pi t}{L}\right)  \right), \end{equation}
where the coefficients $a_{0}, a_{1}, \cdots, b_{1}, b_{2},\cdots$  are given by 
\begin{equation}\label{eq:fsc}
\begin{split}
a_{m} &= \frac{1}{L}\int_{-L}^{L}f(t)\cos\left( \frac{m\pi t}{L}\right) \mathrm{d}t,\quad m=0,1,2,\cdots\\
b_{n} &= \frac{1}{L}\int_{-L}^{L}f(t)\sin\left( \frac{n\pi t}{L}\right) \mathrm{d}t,\quad n=1,2,\cdots
\end{split}
\end{equation}
\begin{theorem}{Convergence.}\label{convergence}

\justify
Suppose that f and its derivative $f^{\prime}$ are piecewise continuous on the interval $-L \leq t \leq L$. Suppose also that $f$ is defined elsewhere so that it is periodic with period $2L$.
Then $f$ has a Fourier series and associated coefficient given by (\ref{eq:fs}) and (\ref{eq:fsc}). Furthermore, the Fourier series converges to $f(t)$ where $f$ is continuous, and to 
\begin{equation}
\frac{1}{2}\left( \lim_{t\rightarrow c^{-}}f(t) + \lim_{t\rightarrow c^{+}}f(t) \right)
\end{equation}
at every point $c$ where $f$ is discontinuous.
\end{theorem}
In vu of Theorem \ref{convergence}, a periodic function and its Fourier series are equal if $f$ is continuous on $-L \leq t \leq L$. If $f$ is discontinuous on $-L \leq t \leq L$ then $f$ and its Fourier series disagree at the discontinuities.
%\begin{itemize}
%\item General formulation
%\item Existence
%\item Uniqueness
%\item Convergence
%\item Stability
%\end{itemize}
%Fourier analysis is concerned with the general problem of periodic and non periodic functions approximation. Fourier series on one hand addresses the former, while the latter is treated by Fourier transform. The basic ingredients required to approximate a function in this scenario are: a vector space, a basis, which is a subspace of the vector space, and a mathematical operation, or more generally a function such as an inner product that maps two vectors to a real number. If a vector space has an inner product, we say that the vector space is an inner product space. 
%\justify
%Before continuing, we see the need to clarify some abbreviations. We use the letters $f$, $V$, $V_{0}$ for an arbitrary function, a vector space, and a subspace of a vector space, respectively.
%Basis functions will be denoted by $\{  \varphi_{0}, \cdots,\varphi_{n}\}$, where $n$ can either be a finite integer or infinite. Having made this clarification, let explain the concept of function approximation. 
%\justify
%The function approximation process in light of Fourier analysis goes like this: Given an arbitrary function $f$ that we seek to approximate, we pick an appropriate vector space which we call $V$, such that $f\in V$. We define a subspace $V_{0}$ of the vector space $V$ and construct an inner product on $V_{0}$, if it does not exist. Furthermore, we fine an appropriate basis of $V_{0}$.  A basis of $V_{0}$ is a set of linearly independent vectors   $\{  \varphi_{0}, \cdots,\varphi_{n}\}$ in $V_{0}$, that span $V_{0}$. This means that any vector in $V_{0}$ can be written as a linear combination of the basis vectors. Once we have all this in place, the best approximation of the function $f$ is its orthogonal projection in the inner product space $V_{0}$. Figure \ref{figure:il} shows an illustration of a generic mechanism of function approximation by orthogonal projection, where $f_{0}$ is the orthogonal projection of $f$ in the subspace $V_{0}$ of $V$.
%\justify
%\begin{figure}[H]
%\begin{center}
%\begin{tikzpicture}
%%\draw (0,0) -- (0,5) -- (0,0) -- (5,0) -- (5,5) -- (0,5);
%%\draw (1,1) -- (1,4) -- (1,1) -- (4,1) -- (4,4) -- (1,4);
%%\draw (0,0) rectangle (7,5);
%%\draw (1,1) rectangle (6,4);
%% reactagle 1
%\path (0,0) coordinate (origin1);
%\path(0.5,4.5) node (V){$V$};
%\path(2.5,0.5) node (f){$\textcolor{blue}{f}\textcolor{blue} {\approx f_{0} = \sum_{j=1}^{n} \alpha_{j} \varphi_{j}}$};
%\path (0,5) coordinate (topleft1);
%\path (7,5) coordinate (topright1);
%\path (7,0) coordinate (bottom1);
%% draw
%\draw (origin1) -- (bottom1) -- (topright1) -- (topleft1) -- (origin1);
%
%% reactagle 2
%\path (1,1) coordinate (origin2);
%\path(1.5,3.5) node (V0){$V_{0}$};
%\path(1.5,1.5) node (f0){$\textcolor{blue}{f_{0}}$};
%
%\path (1,4) coordinate (topleft2);
%\path (6,4) coordinate (topright2);
%\path(4.5,3.5) node (b0){$ \{\varphi_{1}, \varphi_{2}, \cdots, \varphi_{n}\} $};
%\path (6,1) coordinate (bottom2);
%% draw
%\draw (origin2) -- (bottom2) -- (topright2) -- (topleft2) -- (origin2);
%\end{tikzpicture}
%\end{center}
%\caption{Illustration of a generic function approximation process of a function $f$ into a subspace $V_{0}$ of a vector space $V$. $\varphi_{j}$ are the basis functions and $\alpha_{j}$ are real numbers for $j=1,\cdots,n$.}
%\label{figure:il}
%\end{figure}
%\justify
%After explaining the generic process of function approximation, let dive into the specifics. We first tackle Fourier series, which deals with periodic functions. The concepts of vector space, basis, inner product and orthogonal projection are successively define, and we show how they lead to Fourier series expansions of periodic functions. As in any mathematical theory, we also present the well established result of uniqueness of the Fourier series, in conjunction with properties such as stability and convergence. Lastly, to extend function approximation to non-periodic functions, we present Fourier transform as the proper tool. 
%
\subsubsection{Fourier Transform}
\begin{itemize}
\item General formulation
\item Existence
\item Uniqueness
\item Convergence
\item Stability
\end{itemize}
%The Fourier series approximates periodic functions with trigonometric expansions of $\sin$ and $\cos$ functions that have specific properties, inherited from the vector space they belong to.
%Since we are dealing with functions here, function space is more appropriate, because a function space can be a vector space. Let first define a vector space
%%\begin{definition}
%%A vector space $V$ over a field $F$ is a set  with two operations $(\cdot, +)$ satisfying 
%%\begin{itemize}
%%\item Associativity:  (u+v)+w = u+(v+w), for any u,v, w $\in V$
%%\item Commutativity: u+v = v+u, for all u,v $\in V$
%%\item Inverse: for any u $\in V$ there exists an element -u $\in V$ such that u + (-u) = 0
%%\item Identity element: $1\cdot u = u$ where 1 is the multiplicative identity in he field F.
%%\item Distributivity: $a\cdot(u+v) = a\cdot u + a\cdot v$, for any a $\in F$, and u, v $\in V$
%%\item Compatibility $a\cdot(b\cdot u) = (a\cdot b)\cdot u$, for any a,b $\in V$ and u $\in V$
%%\item Scalar distributivity $(a+b)\cdot u = a\cdot u + b\cdot u$, for any a,b $\in F$, and u $\in V$
%%\end{itemize}
%%The elements of the vector space V are called vectors and the element of the field F are call scalars.
%%\end{definition}
%An example of a function space and vector space is the $L^{2}$ space which encompasses square integrable functions. By definition, the set of all squared integrable functions on an interval say $[0,T]$ is denoted by 
%$L^{2}[0,T]$. Any real function $f$ in $L^{2}[0,T]$ satisfies
%\begin{equation} \label{eq:L2}
%\int_{0}^{T}f(t)^{2}\mathrm{d}t < \infty.
%\end{equation}
%Equation (\ref{eq:L2}) says that the total energy of $f$ is finite. This suggest that the $L^{2}$ space is important for real world applications such as signal processing. In fact, we will see  later that any function in $L^{2}$ has a Fourier expansion given in terms of linear combination of sinusoidal basis functions. 
%\justify
%A legitimate question that comes in mind is, how are vectors organised in a vector space. One way to organise vectors in a vector space is to define them in terms of special set of vectors called basis, having well defined properties. Each vector will then be uniquely expressed in terms of the basis vectors. 
%\justify
%\begin{definition}
%A basis is a set of vectors $\varphi_{i}, i = 1, \cdots, n$, covenantally written as 
%\begin{equation}
%\{ \varphi_{1}, \cdots, \varphi_{n} \} \nonumber,
%\end{equation}
%that spans the whole vector space and is linearly independent. Spanning the whole space means that for any vector u $\in V$ there exist unique scalars $a_{i}$ where $i=1,\cdots,n$, such that 
%\begin{equation}
%u = a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n} \nonumber,
%\end{equation}
%and linearly independent means that if
%\begin{equation}
%a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n} = 0 \nonumber,
%\end{equation}
%then 
%\begin{equation}
%a_{1} = \cdots = a_{n} = 0 \nonumber
%\end{equation}
%\end{definition}
%\justify
%With a given basis we can uniquely approximate a vector by decomposing its in terms of the basis vectors. However, a vector space can have several basis, giving rise to several approximations of the same vector. In mathematics and in science in general, one is often concerned with the optimum solution or the unique solution. In terms of an application, uniqueness allows one to pick the relevant physical solution.
%It turns out that, regardless of the basis, the orthogonal projection of a vector is its optimum approximation. Orthogonal projection is achieved by inner product. Let formally define an inner product and show how it leads to orthogonality.
%
%\begin{definition}
%An inner product on a real vector space $V$ is a function 
%\begin{equation}
%\langle\,,\rangle : V\times V \rightarrow \mathbb{R} \nonumber
%\end{equation}
%that satisfies the following properties $\forall u,v \in V$
%\begin{itemize}
%\item Positivity: $\langle\ u,v \rangle > 0$, \quad 
%\item Symmetry: $\langle\ u,v \rangle  = \langle\ u,v \rangle$
%%\item Homogeneity:
%\item Linearity: $\langle\ u+v,w \rangle$ = $\langle\ u,w \rangle +\langle\ v,w \rangle$
%\end{itemize}
%A vector space with an inner product is called an inner product space.
%\end{definition}
%The $L^{2}[0,T]$ space mention previously, is equipped with an inner product.  
%Given two functions $f$ and $g$ in $L^{2}[0,T]$, their inner product is defined as 
%\begin{equation}
%\langle\ f,g \rangle = \frac{1}{T}\int_{0}^{T}f(t)g(t)\mathrm{d}t.
%\end{equation}
%The inner product operation if important because it is used to express orthogonality between two vectors or two functions in the case of a function space.
%
%\begin{definition}\label{ortho}
%Suppose $V$ is an inner product space.
%\begin{itemize}
%\item The vectors $u$ and $v$ are orthogonal if $\langle\ u,v \rangle = 0$
%\item A finite set of vectors $\{ v_{j} \}_{j=1}^{n}$ are orthonormal if each vector have unit length with respect to a norm $||, ||$, that is 
%\begin{equation}
%||v_{j}|| = 1 \nonumber
%\end{equation}
%and $v_{i}$ and $v_{j}$ are orthogonal for $i\neq j$ 
%\end{itemize}
%\end{definition}
%Definition \ref{ortho}, says that two vectors are orthogonal if their inner product is zero and an orthogonal basis is a set of vector which are mutually orthogonal. Once we have an orthogonal basis,
%we can approximate vectors by orthogonal projection.
%
%\begin{theorem}\label{orthogonalityTheorem}
%Suppose $V_{0}$ is a  n-dimensional subspace of an inner product space $V$ and let $\{  \varphi_{j} \}_{j=1}^{n}$ be an orthonormal basis for $V_{0}$, then the orthogonal projection
%of a vector $v\in V$ onto $V_{0}$ is given by 
%\begin{equation}
%v = \sum_{j=1}^{n}\alpha_{j}\varphi_{j}, \quad \text{where}, \quad \alpha_{j} = \langle\ v,\varphi_{j} \rangle \in \mathbb{R}
%\end{equation}
%\end{theorem}
%
%\begin{proof}
%Let 
%\begin{equation}
%v_{0} = \sum_{j=1}^{n}\alpha_{j}\varphi_{j} \nonumber
%\end{equation}
%with 
%\begin{equation}
%\alpha_{j}  = \langle\ v,\varphi_{j} \rangle.  \nonumber
%\end{equation}
%We must show that $v-v_{0}$ is orthogonal to any vector $w \in V_{0}$. Since $\varphi_{1}, \cdots, \varphi_{n}$ is a basis for $V_{0}$, it is enough to show that $v-v_{0}$ is orthogonal to each 
%$\varphi_{k}, k = 1, \cdots, n$.
%\justify
%We have
%\begin{equation}
%\langle\ v-v_{0},\varphi_{k} \rangle = \langle\ v-\sum_{j=1}^{n}\alpha_{j}\varphi_{j},\varphi_{k} \rangle.
%\end{equation}
%Using the fact that $\varphi_{k}, k = 1, \cdots, n$ are orthonormal we get
%\begin{equation}
%\begin{split}
%\langle\ v-v_{0},\varphi_{k} \rangle &= \langle\ v,\varphi_{k} \rangle -\alpha_{k}\langle\ \varphi_{k},\varphi_{k} \rangle \\
%&=\langle\ v,\varphi_{k} \rangle -\alpha, \quad \text{since} \quad || \alpha_{k}|| = 1\\
%&=0, \quad \text{since} \quad \alpha_{k} = \langle\ v,\varphi_{k} \rangle \nonumber
%\end{split}
%\end{equation}
%\end{proof}
%\justify
%From Theorem \ref{orthogonalityTheorem}, We see that a vector can be decomposed in terms of basis functions by orthogonal projection. The orthogonal projection ensures that the decomposition is optimum with respect to a chosen basis, as explained by the following definition
%\begin{definition}
%Suppose $V_{0}$ is a finite-dimensional subspace of an inner product space $V$. For any vector $v \in V$, the orthogonal projection of $v$
%into $V_{0}$ is the unique vector $v_{0} \in V_{0}$ that is closest to $v$, that is,
%\begin{equation}
% ||v-v_{0}|| =  \min_{\forall w \in V_{0}} || v-w ||,
%\end{equation}
%\end{definition}
%\justify
%As mention before, the $L^{2}$ space is important for Fourier series. Recall that a Fourier series of a periodic function is its approximation by trigonometric function. We can now construct orthonormal basis from $L^{2}$ and thus approximate a periodic function by orthogonal projection.
%\begin{theorem}
%The set of functions 
%\begin{equation}
%\left \{ \cdots, \frac{\cos(2x)}{\sqrt{\pi}},\frac{\cos(x)}{\sqrt{\pi}},\frac{1}{\sqrt{\pi}}, \frac{\sin(x)}{\sqrt{\pi}},\frac{\sin(2x)}{\sqrt{\pi}},\cdots  \right \}
%\end{equation}
%is an orthonormal set in $L^{2}([-\pi,\pi])$ space
%\end{theorem}
%\begin{proof}
%The proof is given by the following theorem bellow
%\end{proof}
%
%\begin{theorem}{Theorem}
%The following integral relations hold
%\begin{equation}
%\begin{split}
% \frac{1}{\pi}\int_{-\pi}^{\pi} \cos(nx)\cos(kx) \mathrm{d}t =  \begin{cases}
%  1 & \text{for } n=k \geq 1\\    
%  2 & \text{for } n = k = 0\\
%  0 & \text{otherwise}  
%\end{cases}  \nonumber 
%\end{split}
%\end{equation}
%
%\begin{equation}
%\begin{split}
% \frac{1}{\pi}\int_{-\pi}^{\pi} \sin(nx)\sin(kx) \mathrm{d}t =  \begin{cases}
%  1 & \text{for } n=k \geq 1\\    
%  0 & \text{otherwise }  
%\end{cases}  \nonumber 
%\end{split}
%\end{equation}
%
%\begin{equation}
%\begin{split}
% \frac{1}{\pi}\int_{-\pi}^{\pi} \cos(kx)\sin(kx) \mathrm{d}t = 0
%\end{split}
%\end{equation}
%\end{theorem}
%
%\begin{proof}
%We use the following trigonometric identities 
%\begin{equation}
%\begin{split}
%\cos((n+k)x) &= \cos(nx)\cos(kx) - \sin(nx)\sin(kx)\\
%\cos((n-k)x) &= \cos(nx)\cos(kx) + \sin(nx)\sin(kx) \nonumber
%\end{split}
%\end{equation}
%to get 
%\begin{equation}
%\begin{split}
%\int_{-\pi}^{\pi} \cos(nx)\cos(kx) &= \frac{1}{2}\int_{-\pi}^{\pi}(\cos(n+k)x + \cos(n-k)x)\mathrm{d}x \\
%&=\frac{1}{2}\left [  \frac{\sin(n+k)x}{n+k} + \frac{\sin(n-k)x}{n-k} \right ]_{-\pi}^{\pi}\\
%&=\begin{cases}
%  1 & \text{for } n=k \geq 1\\    
%  2 & \text{for } n = k = 0\\
%  0 & \text{otherwise}  
%\end{cases}  \nonumber 
%\end{split}
%\end{equation}
%
%\begin{equation}
%\begin{split}
%\int_{-\pi}^{\pi} \sin(nx)\sin(kx) &= \frac{1}{2}\int_{-\pi}^{\pi}(-\cos(n+k)x + \cos(n-k)x)\mathrm{d}x \\
%&=\frac{1}{2}\left [  \frac{-\sin(n+k)x}{n+k} + \frac{\sin(n-k)x}{n-k} \right ]_{-\pi}^{\pi}\\
%&=\begin{cases}
%  1 & \text{for } n=k \geq 1\\    
%  0 & \text{otherwise}  
%\end{cases}  \nonumber 
%\end{split}
%\end{equation}
%We know that the integral of a function in $[-\pi, \pi]$ is zero. Furthermore since
%\begin{equation}
%\cos(-nx)\sin(-kx) = - \cos(-nx)\sin(-kx) \nonumber
%\end{equation}
%is odd we get 
%\begin{equation}
% \frac{1}{\pi}\int_{-\pi}^{\pi} \cos(kx)\sin(kx) \mathrm{d}t = 0 \quad \text{for } k\geq 0
%\end{equation}
%\end{proof}
%\justify
%Having define trigonometric basis, we can now formally give the Fourier series of a periodic function.
%\begin{definition}
%The Fourier series of a periodic function $g$ is given by the expansion 
%\begin{equation}
%g(x) = a_{0} + \sum_{k=1}^{n}a_{k}\cos(kx)+b_{k}\sin(kx)
%\end{equation}
%where $a_{k}, k=0,\cdots, n$ are called the Fourier coefficients of the function $g$
%\end{definition}
%
%\begin{theorem}
%if 
%\begin{equation}
%g(x) = a_{0} + \sum_{k=1}^{n}a_{k}\cos(kx)+b_{k}\sin(kx)
%\end{equation}
%then 
%\begin{equation}
%a_{0} = \frac{1}{2\pi}\int_{-\pi}^{\pi}g(x)\mathrm{d}x
%\end{equation}
%\begin{equation}
%a_{k} = \frac{1}{2\pi}\int_{-\pi}^{\pi}g(x)\cos(kx)\mathrm{d}x
%\end{equation}
%\begin{equation}
%b_{k} = \frac{1}{2\pi}\int_{-\pi}^{\pi}g(x)\sin(kx)\mathrm{d}x
%\end{equation}
%\end{theorem}
%
%\begin{proof}
%(need a proof here) We use the orthogonality property to derive the Fourier coefficients.
%\end{proof}
%
%




\subsubsection{Fourier transform}








%\subsubsection{Application to bearing fault detection}
%In this approach we use Fourier transform to detect defect frequency such outer race defect frequency and inner race defect frequency in the envelop spectrum. In this case the envelop spectrum is the new feature generated from the original vibration data. The bearing type used are Rexnor ZA-2115. For this type of bearing, the constant defect frequency (cdf) for ball pass frequency outer race defect is 0.1182 and the constant defect frequency for ball pass frequency inner race defect is 0.1484 [reference]. According to Rexnord product engineering group [reference], to find the defect frequency in Hz we multiply the constant defect frequency (cdf) by the rotational speed of the bearing ( here 2000 RPM), to find the defect frequency in Hz. Note that the defect frequency is computed from the geometry of the bearing, which means that for a specific bearing, this is a constant value. 
%
%\begin{flushleft}
%Figure \ref{fig:bearing} shows the geometry of a bearing, from which bearing defect frequency can be computed. Figure \ref{fig:decomp}  illustrates how Fourier transform can decompose vibration data into its frequency components. Each frequency component which are trigonometric functions are characterised by constant frequency and amplitude. In a defect bearing, the defect frequency will be coupled with a relatively high amplitude. 
%\end{flushleft}
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4in]{bearing.png} 
%   \caption{Geometry of a bearing}
%   \label{fig:bearing}
%\end{figure}
%\begin{flushleft}
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4in]{decomposition} 
%   \caption{Illustration of using Fourier transform to decompose a time signal to its frequency components}
%   \label{fig:decomp}
%\end{figure}
%
%Next, we compute the envelop spectrum for each vibration signal by using the FFT method (Fast Fourier Transform). The envelop spectrum is used to detect early sign of failure. Recall that a vibration signal can be decomposed into its sub components, where each  sub component 
%is characterised by its frequency and its amplitude. Early sign of failure can be seen in the high frequency low amplitude sub component. As the failure becomes more pronounced, it becomes visible in the low frequency high amplitude sub component. Figure \ref{fig:signal0} shows the raw vibration signal, the corresponding envelop spectrum and the ball pass frequency outer race defect.
%\end{flushleft}
%
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4in]{time-signal.png} 
%   \includegraphics[width=4in]{spectrum.png} 
%      \includegraphics[width=4in]{fault.png} 
%   \caption{Vibration time signal}
%   \label{fig:signal0}
%\end{figure}
%\begin{flushleft}
%By computing the envelop spectrum for each time signal and extracting the amplitude of a defect frequency we can visualise the severity of a defect. Figure \ref{fig:signal111} and \ref{fig:signal221} show the amplitude of outer race defect and inner race defect for four bearings and three bearings respectively. In Figure \ref{fig:signal111} we can observe that bearing number one is affected severely by ball pass frequency outer race defect. Figure \ref{fig:signal222} shows that bearing number three has ball pass frequency inner race defect. 
%\end{flushleft}
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=7in]{signal_processing_method_bpfo} 
%   \caption{Ball pass frequency outer race detection}
%   \label{fig:signal111}
%\end{figure}
%
%\begin{figure}[H] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=7in]{signal_processing_method_bpfi} 
%   \caption{Ball pass frequency inner race detection}
%   \label{fig:signal222}
%\end{figure}
%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wavelet transform}
\subsubsection{Theory}
\subsubsection{Application}
In the wavelet transform we generate two extra features from the vibration time signal, namely the discrete detailed coefficient cD and the approximate coefficient cA. The detail coefficient cD represents the hight frequency component of the vibration time signal and the approximate coefficient cA represents  
the low frequency component.  For the mother wavelet we use Daubechies 20 or db20 shown in Figure \ref{fig:wavelet}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{dbscaling}
    \includegraphics[width=2in]{dbwavelet}  
   \caption{The scaling function $\varphi$ (left) and the wavelet function $\Psi$ (right) }
   \label{fig:wavelet}
\end{figure}

% A pseudo code in python to implement the method:
%\begin{python}
%pip install pywavelet
%\end{python}
%\begin{python}
%import pywt
%import numpy as np
%container = []
%mother_wavelet = 'db20'
%ref_cA, ref_cD = pywt.dwt(ref_sample, mother_wavelet)
%for sample in samples:
%	cA, cD = pywt.dwt(ref_sample, mother_wavelet)
%	cA_dissimilarity = dissimilarity_measure(ref_cA,cA)
%	cD_dissimilarity = dissimilarity_measure(ref_cD,cD)
%	container.append((cA_dissimilarity,cD_dissimilarity))
%\end{python}
\begin{flushleft}
Once we have the two extra features, we compute the dissimilarity between a reference sample and subsequent samples for each feature. 
This process generates a set of points $(x,y)$ that represent the health index of each sample. From Figure \ref{fig:wo} and \ref{fig:wi}, we can observe that bearing number four and bearing number three suffer from ball pass frequency outer race and ball pass frequency inner race defect respectively. We can also observe that a bearing can go through three main stages:
\begin{enumerate}
\item A healthy stage characterised by a low health index
\item A warning stage characterised by an increasing health index
\item An alarm stage characterised by a high health index
\end{enumerate}


\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{mixed_method_bpfo.png} 
   \caption{Ball past frequency outer race defect detection from wavelet transform}
   \label{fig:wo}
\end{figure}
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{mixed_method_bpfi.png} 
   \caption{Ball past frequency outer race defect detection from wavelet transform}
   \label{fig:wi}
\end{figure}
In the alarm stage, as the degradation becomes more severe, the distance between the points increases.
\end{flushleft}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hilbert Huang transform}
\subsubsection{Theory}
The Hilbert-Huang transform is a data decomposition methods that consists of decomposing data in an adaptive fashion. Adaptivity means that rather than imposing an a priory basis such as trigonometric functions, a posteriori basis functions are derived from the data itself \cite{norden2008}. In doing so, the method deals better with nonlinearity and non stationarity which are inherently present in real world data.

\begin{flushleft}
This method gives an alternative approach of time-frequency-energy paradigm by using Hilbert spectral analysis and the so call empirical mode decomposition (EMD) to express the nonlinearity and the non stationary in data 
with instantaneous frequency and instantaneous amplitude \cite{norden2008}.
\end{flushleft}
\begin{flushleft}
The empirical mode decomposition (EMD) originated from the quest of functions that can be expressed by a time-frequency-amplitude expression, such that the frequency is physically meaningful.
Consider a time series $x(t)$. Its Hilbert transform H(t) is given by 

\begin{equation}
H(t) = \frac{1}{\pi}P\int_{_\infty}^{\infty}\frac{x(\tau)}{t-\tau}\mathrm{d}\tau,
\end{equation}
where $P$ is the Cauchy principal value. The corresponding time-frequency-amplitude function of $x(t)$ is the analytical function
\begin{equation}
z(t) = x(t) + iy(t) = a(t)e^{i\theta(t)},
\end{equation}
where the instantaneous amplitude $a(t)$ and phase $\theta(t)$ can be computed by 
\begin{equation}\label{eq:amp}
a(t) = \sqrt{x(t)^{2}+ y(t)^{2}}
\end{equation}
\begin{equation}
\theta(t) = \tan^{-1}\left(\frac{y(t)}{x(t)}\right).
\end{equation}
Furthermore, the instantaneous frequency $w(t)$ can be derived from the phase $\theta(t)$ as
\begin{equation}\label{eq:freq}
w(t) = \frac{d\theta}{dt}.
\end{equation}
By setting 
\begin{equation}
f(t) = \frac{y(t)}{x(t)}, \nonumber
\end{equation}
the expression of the instantaneous  amplitude $w(t)$ in (\ref{eq:freq}) can be expanded as
\begin{equation}
w(t) = \frac{f^{\prime}(t)}{1+f(t)^{2}} = \frac{y^{\prime}(t)x(t)-y(t)x^{\prime}(t)}{x(t)(x(t)+y(t)^{2})}.
\end{equation}
The instantaneous frequency $w(t)$ using the Hilbert transform is not always physically meaning. For example for an arbitrarily function, the instantaneous physical frequency values should be positive. However this is not always the case. 

\begin{flushleft} 
For example if 
\begin{equation}
f(x) = \cos(ct) +d 
\end{equation}
where $c$ and $d$ are constants, the instantaneous frequency is given by
\begin{equation}
w(t) = \frac{-c\sin(ct)}{1+(\cos(ct) +d)^{2} }
\end{equation}
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2.5in]{f} 
     \includegraphics[width=2.5in]{frequency} 
   \caption{function f and its corresponding frequency}
   \label{fig:freq}
\end{figure}
From Figure \ref{fig:freq}, we see that the instantaneous frequency takes negative values, which is not physically meaning full. 
\end{flushleft}

\begin{flushleft}
To circumvent this, the Hilbert-Huang transform offers a methodology to obtain from an arbitrarily function or time series $x(t)$ a set of finite subcomponents whose instantaneous frequency are physically meaningful. This methodology let to the empirical mode decomposition.
\end{flushleft}



%Once the instantaneous frequency and amplitude are defined, we can define the Hilbert-Huang spectrum H(w,t), \cite{hui2007} as 
%\begin{equation}\label{eq:hs}
%H(w,t) = Re\sum_{i=1}^{n}a_{i}(t)\exp\left(\int w_{i}(t)\mathrm{d}t\right),
%\end{equation}
%and compute the marginal spectrum 
%\begin{equation}
%h(w) = \int_{0}^{T}H(w,t)\mathrm{d}t.
%\end{equation}
%While the Hilbert spectrum is the energy contribution of each instantaneous frequency, the marginal spectrum is the total energy contribution of all instantaneous frequency
%\cite{norden2008, hui2007}. 
\end{flushleft}

\begin{flushleft}
 The necessary condition for obtaining a physical frequency is that $x(t)$ satisfies the approximate local envelope symmetry condition \cite{norden1998}. 



This condition 
is expressed in the empirical mode decomposition (EMD) such that an arbitrarily time series $x(t)$ can be decomposed by a sifting process into intrinsic mode function $c_{i}$
\begin{equation}
x(t) = \sum_{i=1}^{n}c_{i} + r_{n}
\end{equation}
where the $c_{i}$ satisfies the approximate local envelope symmetry condition
\begin{equation}
SD_{k} = \frac{\sum_{t=0}^{T}}{\sum_{t=0}^{T}} < \epsilon
\end{equation}
where $\epsilon$ is a small predefined real number.
\end{flushleft}

\subsubsection{Application for bearings fault detection}
we consider a vibration signal with sample frequency of 20000Hz rotating speed of 2000 RPM
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{signal} 
   \caption{Vibration signal of 1 second snapshot}
   \label{fig:signal}
\end{figure}
\begin{flushleft}
After applying the empirical mode decomposition on the vibration data from figure \ref{fig:signal} we get sixteen  intrinsic mode functions
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2.5in]{imf/imf1.png} 
     \includegraphics[width=2.5in]{imf/imf2.png} 
   \caption{1th and 2nd intrinsic mode function (imf)}
   \label{fig:imf11}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf3.png} 
     \includegraphics[width=2in]{imf/imf4.png} 
   \caption{3rd and 4th intrinsic mode function (imf)}
   \label{fig:imf34}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf5.png} 
     \includegraphics[width=2in]{imf/imf6.png} 
   \caption{5th and 6th intrinsic mode function (imf)}
   \label{fig:imf56}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf7.png} 
     \includegraphics[width=2in]{imf/imf8.png} 
   \caption{7th and 8th intrinsic mode function (imf)}
   \label{fig:imf78}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf9.png} 
     \includegraphics[width=2in]{imf/imf10.png} 
   \caption{9th and 10th intrinsic mode function (imf)}
   \label{fig:imf910}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf11.png} 
     \includegraphics[width=2in]{imf/imf12.png} 
   \caption{11th and 12th intrinsic mode function (imf)}
   \label{fig:imf1112}
\end{figure}

\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{imf/imf13.png} 
     \includegraphics[width=2in]{imf/imf14.png} 
   \caption{13th and 14th intrinsic mode function (imf)}
   \label{fig:imf1314}
\end{figure}

\end{flushleft}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Machine learning methods   \textcolor{red}{(TO DO!!)}}
\subsection{Overview   \textcolor{red}{(TO DO!!)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Machine learning and signal processing: Case study}
\subsection{Overview}
In this section we show three case study where we use our propose methodology for anomaly detection in predictive maintenance.
\subsection{Bearing fault detection}
\subsubsection{Problem formulation}
Four bearings installed on the shaft of a motor exhibit different health conditions. The goal is to be able to use a machine learning model to detect the fault and prevent subsequent failures.
Figure \ref{fig:exp} shows the experimental setup with two accelerometers installed per bearing, measuring radial and axial vibration. An accelerometer is a sensor that measures the vibration of a given system. The radial vibration is the vibration experienced by the shaft perpendicular to it axis, while the axial vibration is the vibration experienced by the shaft along it axis.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{thesis-pictures/experiment} 
   \caption{Experimental setup from \cite{hai}}
   \label{fig:exp}
\end{figure}
\justify
The shaft is subjected to a radial load of 6000 Pound-force and the motor is rotating at 2000 RPM (Rotation Per Minute) or 33.33 Hz \cite{ims}. The bearings used are of type Rexnord ZA-2115.

\subsubsection{Conceptual model}
Figure \ref{fig:exp} shows a very good description of the system and can serve as a conceptual model. Recall that the conceptual model is a diagram of the system explaining its behaviour.
In this case the four bearings mounted on the shaft are subjected to a radial load of 6000 Pound-force or equivalently $26689.32$ Newton. As the motor is rotating, this axial load will be will eventually cause the bearing to fail after a period of time.
\justify
We are interested in detecting failure in the four bearings if they occur or as early as possible. The common failure type are inner race defect, outer race defect, roller element defect, cage defect. Figure \ref{fig:bearing-schema} shows a schematic description of a bearing. The inner ring is in contact with the shaft. The cage holds in place the roller elements which are spherical elements. As the shaft rotates, the roller elements, the cage, the inner race and the outer race are all under load.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{thesis-pictures/bearing} 
   \caption{Schematic description of a bearing. Taken from [ref]}
   \label{fig:bearing-schema}
\end{figure}
\justify
For inner race defect, a crack occurs in the inner ring, while for outer race a crack occurs at the outer ring. Similarly, cage and rolling element defect are defect happening at the cage and roller elements.
\subsubsection{Data description}
Three datasets resulting from three different experiment are available. In the first experiment run from October 22, 2003 at 12:06:24 to November 25, 2003 at 23:39:56, each of the four bearing axial and radial vibration were measured every 10 minutes. At the end of the experiment, bearing 3 exhibited inner race defect while bearing 4 exhibited roller element defect.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=7in]{thesis-pictures/bearing1-test1} 
   \caption{Axial vibration measurement for the first test}
   \label{fig:example}
\end{figure}
\justify
The second experiment run from February 12, 2003 10:32:39 to February 19, 2004 06:22:39.  Radial acceleration were measured on bearing 1, 2,3 and four. The measurement were done every 10 minutes. At the end of the experiment, outer race defect occurred in bearing 1.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=7in]{thesis-pictures/bearing1-test2} 
   \caption{Axial vibration measurement for the second test}
   \label{fig:example}
\end{figure}
\justify
The third experiment run from March 4, 2004 09:27:46 to April 4, 2004 19:01:57 and the measurement were done every 10 minutes. At the end of the experiment, outer race defect occurred in bearing 3.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=7in]{thesis-pictures/bearing1-test3} 
   \caption{Axial vibration measurement for the third test}
   \label{fig:example}
\end{figure}

\subsubsection{Data driven model}
The objective now is to apply Fourie






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion   \textcolor{red}{(TO DO!!)}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{9}
\bibitem{norden2008} 
Norden E. Huang and Zhaohua Wu 
\textit{A review on Hilbert-Huang Transform: Methods and its Application to Geophysical Studies}. 
Review of Geophysics, 2008
 
\bibitem{norden1998} 
Norden E. Huang, Zheng Shen, Steven R. Long, Manlic C. Wu, Hsing H.Shih, Quanan Zheng, Nai-Chyuan Yen, Chi Chao Tung and Henry H. Liu
\textit{The Empirical mode decomposition and the Hilbert Spectrum for nonlinear and non-stationary time series analysis}
Proc. R. Soc. Lond. A (1998) 455, 903:995.

\bibitem{chandola} 
Varun Chandola, Arindam Banerjee, and Vipin Kumar.
\textit{Anomaly detection: A survey}
Proc. R. Soc. Lond. A (1998) 455, 903:995.

\bibitem{hai} 
Hai Qiu, Jay Lee, Jing Lin
\textit{Wavelet Filter-based Weak Signature Detection Method and its Application on Roller Bearing Prognostics}
Journal of Sound and Vibration 289 (2006) 1066-1090

\bibitem{ims} 
\textit{IMS Bearing Data}



 
\bibitem{pwc} 
Michel Mulders and Mark Haarman
\textit{Predictive maintenance 4.0. Predict the unpredictable, June 2017}

 
\bibitem{hui2007} 
Hui Li, Yuping Zhang and Haiqi Zheng
\textit{Hilbert-Huang transform and marginal spectrum for detecting and diagnosis of localized defects in roller bearings}
Journal of Mechanical Technology, 2007
\end{thebibliography}






\end{document}  