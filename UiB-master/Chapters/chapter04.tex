% !TEX encoding = UTF-8 Unicode
%!TEX root = ../Main/thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\documentclass[../Main/thesis.tex]{subfiles}
\begin{document}
\chapter[One class SVM coupled with wavelet transform for bearing health monitoring]{One class SVM coupled with wavelet transform for bearing health monitoring}
\label{sec:waveletandsvm}
In this chapter we introduce yet a another method for bearing fault detection. In Chapter \ref{sec:chapter2} We showed how the fast Fourier transform is applied to bearing fault diagnosis. Although popular in the industry for bearing fault analysis and detection, the fast Fourier transform method is not without limitations. As in any scientific methods, the limitations are grounded in the mathematical assumptions upon which the method is derived. As pointed previously, two such assumption are linearity, and stationarity. Furthermore, for bearing fault analysis, the fast Fourier method relies on the rotational speed of the motor on which the bearings are mounted. In addition, the forcing frequencies are derived from the physical properties of the bearing. Thus, the fast Fourier transform method for bearing fault detection can be seen as a physics base data driven method.
\justify
In this chapter, a purely data driven method is presented. It consists of applying wavelet transform for feature generation, a robust statistical estimator to quantify a bearing health, and a support vector machine (SVM) for anomaly detection. Before giving a detail account of the method, let clarify some semantics and outline the benefits of this method. A feature in the context of this chapter, is a component of a signal. As pointed out earlier, a signal can have multiple components, which can be extracted for example by applying Fourier series or Fourier transform. In this case a feature or component of the original signal is a trigonometric function. A support vector machine (SVM) is an algorithm for pattern identification that uses margins to separate data into different groups [vapnik1995].
\justify
The method presented in this chapter solely relies on the data to detect any fault. it does not require bearing physical properties, and can be applies to any nonlinear and non-stationary data generated process. In the rest of this chapter we will refer to a generic bearing fault as an anomaly. As defined in [chandola2009], \say{Anomalies are patterns in data that do not conform to a well defined notion of normal behavior}. In section \ref{sec:svm} a detailed account of support vector machine algorithm is lade out, followed by wavelet transform in section \ref{sec:wavelet}. Finally, section \ref{sec:sectionresult} presents a one class SVM coupled with wavelet transform for bearing anomaly detection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Support vector machine (SVM)}
\label{sec:svm}
The Support vector machine algorithm was originally derived by [vopnikk1995] as a two-groups, classification algorithm. It solves the problem of classifying an input vector into two groups. In the context of bearing fault detection: healthy versus unhealthy bearing would be the two groups. It was originally introduced as support-vector Networks, but is now popularly refer to as support vector machine. The bulk of the SVM rests on the following idea: Use a nonlinear kernel to map the input data into a high dimensional feature space, where a linear decision function is used to classify data into two groups [vopmik1995].
\justify
A non linear kernel is just a nonlinear function and a decision function also called a discriminator function is a curve that separates the two groups. The classification is based on optimizing a margin separating the two groups. 
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{../fig/svm2d.png} 
   \caption{Illustration of two classes classification from [vapnik1995]}
   \label{fig:svm2d}
\end{figure}
\justify
Figure \ref{fig:svm2d} shows a two groups classification problem. Two set of points marked by circle and cross are separated by the optimum curve or hyperplane (here the solid dash line). The margin is the distance or gap between solid dark lines. Here the two classes are linearly separable, meaning that a linear function can be used to separate the two classes. Support vectors are points that lies on the margins (points in grey square lying on the dark lines) and defines the margin of largest separation between the two classes [vapnik1995].
\justify
The SVM can be formulated as a convex quadratic optimization problem. In this setting, the objective function is convex and quadratic, and the inequality constraint is convex. Henceforth, from the duality principal of the mathematical optimization theory, there exist two formulations of the SVM: A primal formulation and an equivalent dual formulation. For convex optimization problem, we always nearlly have strong duality, that is the optimum solution of the primal problem is equal to the optimum solution of the dual problem.

\justify
The formulation of the SVM problem goes like this: Find the optimum linear curve (the dash line in Figure \ref{fig:svm2d}), that separates  two groups of points (circle points and cross points in Figure \ref{fig:svm2d}), from the nearest points of each class (the support vectors which are points in the in grey square in Figure \ref{fig:svm2d}).
\justify
Let $\bm{X}_{i} \in \mathbb{R}^{d}$  be an input vector, $\bm{W}\in \mathbb{R}^{d}$ a vector of parameters and $b\in \mathbb{R}$, where $d$ is an integer. Let $y_{i}\in \{-1,1\}$. Any vector $\bm{X}_{i}$ can be classify as 
\begin{equation}\label{eq:svmformulation}
y_{i} =
  \begin{cases}
                                   -1 & \text{if $\bm{W}^{T}\bm{X}_{i} + b < 0$} \\
                                   1 & \text{if $\bm{W}^{T}\bm{X}_{i} + b > 0$},
  \end{cases}
\end{equation}
%\begin{equation}\label{eq:svmformulation}
%\begin{split}
%\bm{W}^{T}\bm{X}_{i} + b &< 0, \quad \Rightarrow y_{i} = -1\\
%\bm{W}^{T}\bm{X}_{i} + b &> 0, \quad \Rightarrow y_{i} = 1,
%\end{split}
%\end{equation}
where the equation of the separating hyperplane is given by 
\begin{equation}\label{eq:svmformulation01}
\bm{W}^{T}\bm{X}_{i} + b = 0.
\end{equation}
The problem specified by equation (\ref{eq:svmformulation}, \ref{eq:svmformulation01}) can be formulated as the following convex quadratic optimization problem:
\begin{equation}\label{eq:svmformulation1}
\min_{\bm{W},b}\frac{1}{2}||  \bm{W}||^{2}
\end{equation}
subjected to 
\begin{equation}\label{eq:svmformulation2}
y_{i}\left( \bm{W}^{T}\bm{X}_{i} + b \right) \geq 0, \quad i = 1,\cdots,n.
\end{equation}
Equations (\ref{eq:svmformulation1}, \ref{eq:svmformulation2}) is called the primal formulation of the SVM. An equivalent dual formulation called the Lagrangian dual formulation is defined as followed:
Let 
\begin{equation}\label{eq:svmprime1}
L(\bm{W}, b, \theta) = \frac{1}{2}||  \bm{W}||^{2} -\sum_{i}^{n}\theta_{i}\left(y_{i}\left( \bm{W}^{T}\bm{X}_{i} + b \right) -1  \right)
\end{equation}
be the Lagrangian derived from the primal SVM formulation, where $\theta_{i} \in \mathbb{R}^{d}$, and $\bm{W}^{T}$ denotes the transpose of the vector $\bm{W}$. Taking the partial derivative of the Lagrangian with respect to $\bm{W}$ and $b$ we obtain  
\begin{equation}\label{eq:dual1}
\begin{split}
\frac{\partial L(\bm{W}, b, \theta)}{\partial \bm{W}} &= \bm{W}-\sum_{i=1}^{n}\theta_{i}y_{i}\bm{X}_{i} = 0, \quad \Rightarrow \bm{W} = \sum_{i=1}^{n}\theta_{i}y_{i}\bm{X}_{i}\\
\frac{\partial L(\bm{W}, b, \theta)}{\partial b} &= \sum_{i=1}^{n}\theta_{i}y_{i} = 0.
\end{split}
\end{equation}
Substituting $\bm{W}$ back to the primal formulation (\ref{eq:svmformulation1}, \ref{eq:svmformulation2}) , with $ \sum_{i=1}^{n}\theta_{i}y_{i} = 0$, we obtain the dual Lagrangian formulation as 
\begin{equation}
\max_{\theta}\sum_{i=1}^{n}\theta_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\theta_{i}\theta_{j}y_{i}y_{j}\bm{X}_{i}\bm{X}_{j}
\end{equation}
subjected to 
\begin{equation}
\begin{split}
\sum_{i}^{n}\theta_{i}y_{i} = 0\\
\theta_{i} \geq 0, \quad \forall i = 1,\cdots,n
\end{split}
\end{equation}
which can be efficiently solve with a sequential minimization algorithm. The above primal and dual formulations hold for linear separable classes. \begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{../fig/nonlinearsvm.png} 
   \caption{Nonlinear separable points in two dimensions}
   \label{fig:nonlinearsvm}
\end{figure}
\justify
Figure \ref{fig:nonlinearsvm} illustrates a nonlinear separable problem, where the two classes can not directly separated by a linear curve. This problem requires a different formulation. In this case a nonlinear function $K(X,Z)$
\begin{equation}
K: \mathbb{R}^{d}\times \mathbb{R}^{d} \rightarrow \mathbb{R}
\end{equation}
called a kernel function is required. The latter maps the data into a higher dimension space, where the data is linearly separable. This is illustrated in Figure \ref{fig:kernelsvm} where  class 1 and 2 originally nonlinearly separable, are map to a higher dimension where they are linearly separable.
\begin{figure}[H] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{../fig/kernelsvm.png} 
   \caption{Using a kernel to map the data in higher dimension in order to linearly separate class 1 and 2}.
   \label{fig:kernelsvm}
\end{figure}
\justify
Example of common Kernel functions are 
\begin{itemize}
\item Linear kernel,  $K\left(X,Z\right) = X^{T}Z$
\item Polynomial of degree $q$ kernel, $K\left(X,Z\right) = \left(1+X^{T}Z\right)^{q}$ 
\item Exponential kernel,  $K(X,Z) = \exp\left(-\gamma ||X-Z||\right)$ ,$\gamma\in\mathbb{R}$
\item Gaussian kernel,  $K(X,Z) = \exp\left(-\gamma ||X-Z||^{2}\right)$ ,$\gamma\in\mathbb{R}$
\end{itemize}
The nonlinear dual formulation of the SVM can now be formulated in term of the kernel function $K$ as 
\begin{equation}
\max_{\theta}\sum_{i=1}^{n}\theta_{i} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\theta_{i}\theta_{j}y_{i}y_{j}K\left(\bm{X}_{i},\bm{X}_{j}\right)
\end{equation}
subjected to 
\begin{equation}
\begin{split}
\sum_{i}^{n}\theta_{i}y_{i} = 0\\
\theta_{i} \geq 0, \quad \forall i = 1,\cdots,n.
\end{split}
\end{equation}
\justify
Some of the advantages of the SVM are high classification accuracy and robustness to outliers. That is it deals very well with aberrant data points as well as missing data points. However, for nonlinear separable problems, the computation of the kernel can incur significant overhead for large data, since the complexity is $\mathcal{O}\left(n^{2}\right)$. This can however be mitigated by perfoming the so call kernel trick, which reduces the complexity of the computation. Detail account of the kernel trick can be found in [reference].
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wavelet transform}
\label{sec:wavelet}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Coupled SVM and wavelet transform for bearing anomalies detection}
\label{sec:sectionresult}


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\blankpage
\end{document}

